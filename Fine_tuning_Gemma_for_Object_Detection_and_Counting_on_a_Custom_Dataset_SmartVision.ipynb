{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNKGUQB24KiMoF79b2DuK/M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DATAGEEKN/SmartVision-Counter/blob/main/Fine_tuning_Gemma_for_Object_Detection_and_Counting_on_a_Custom_Dataset_SmartVision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "from transformers import PaliGemmaForConditionalGeneration, AutoProcessor, BitsAndBytesConfig, Trainer, TrainingArguments\n",
        "from peft import get_peft_model, LoraConfig\n",
        "from huggingface_hub import login\n",
        "from zipfile import ZipFile\n",
        "import logging\n",
        "from getpass import getpass\n",
        "\n",
        "# Install dependencies\n",
        "try:\n",
        "    import googletrans\n",
        "    from googletrans import Translator, LANGUAGES\n",
        "except ImportError:\n",
        "    !pip install googletrans==4.0.0-rc1\n",
        "    try:\n",
        "        from googletrans import Translator, LANGUAGES\n",
        "    except ImportError:\n",
        "        !pip install translate\n",
        "        from translate import Translator as FallbackTranslator\n",
        "        logger.warning(\"Using fallback translate library due to googletrans installation failure\")\n",
        "\n",
        "# Set up logging for demo appeal\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Step 1: Prompt for Hugging Face token\n",
        "def get_hf_token():\n",
        "    token = getpass(\"Enter your Hugging Face access token: \")\n",
        "    if not token.strip():\n",
        "        raise ValueError(\"Hugging Face token cannot be empty. Please provide a valid token.\")\n",
        "    return token\n",
        "\n",
        "try:\n",
        "    HF_TOKEN = get_hf_token()\n",
        "    login(token=HF_TOKEN, add_to_git_credential=False)\n",
        "    logger.info(\"Hugging Face login successful\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error during Hugging Face login: {e}\")\n",
        "    raise\n",
        "\n",
        "# Step 2: Set up environment\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info(f\"Using device: {device}\")\n",
        "\n",
        "# Step 3: Create directories\n",
        "os.makedirs(\"/content/coco_images\", exist_ok=True)\n",
        "os.makedirs(\"/content/coco_processed\", exist_ok=True)\n",
        "os.makedirs(\"/content/paligemma_finetuned\", exist_ok=True)\n",
        "\n",
        "# Step 4: Download and extract COCO 2017 validation annotations\n",
        "anno_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
        "anno_zip = \"annotations_trainval2017.zip\"\n",
        "anno_path = \"/content/coco_annotations/annotations/instances_val2017.json\"\n",
        "if not os.path.exists(anno_path):\n",
        "    logger.info(\"Downloading annotations...\")\n",
        "    response = requests.get(anno_url, stream=True)\n",
        "    with open(anno_zip, \"wb\") as f:\n",
        "        for chunk in tqdm(response.iter_content(chunk_size=1024), desc=\"Downloading annotations\"):\n",
        "            f.write(chunk)\n",
        "    with ZipFile(anno_zip, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(\"/content/coco_annotations\")\n",
        "\n",
        "# Step 5: Load COCO annotations\n",
        "try:\n",
        "    with open(anno_path, \"r\") as f:\n",
        "        coco_data = json.load(f)\n",
        "    logger.info(\"COCO annotations loaded successfully\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error loading COCO annotations: {e}\")\n",
        "    raise\n",
        "\n",
        "# Step 6: Map category IDs to names\n",
        "category_map = {cat[\"id\"]: cat[\"name\"] for cat in coco_data[\"categories\"]}\n",
        "\n",
        "# Step 7: Process COCO images for PaliGemma\n",
        "paligemma_data = []\n",
        "image_filenames = [\"000000289343.jpg\", \"000000039769.jpg\"]\n",
        "for img in coco_data[\"images\"][:50]:\n",
        "    if img[\"file_name\"] not in image_filenames:\n",
        "        image_filenames.append(img[\"file_name\"])\n",
        "\n",
        "for filename in tqdm(image_filenames, desc=\"Processing images\"):\n",
        "    img_path = f\"/content/coco_images/{filename}\"\n",
        "    image_url = f\"http://images.cocodataset.org/val2017/{filename}\"\n",
        "\n",
        "    if not os.path.exists(img_path):\n",
        "        try:\n",
        "            img_data = requests.get(image_url, timeout=10).content\n",
        "            with open(img_path, \"wb\") as f:\n",
        "                f.write(img_data)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to download {filename}: {e}\")\n",
        "            continue\n",
        "\n",
        "    try:\n",
        "        Image.open(img_path).verify()\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Invalid image {filename}: {e}\")\n",
        "        continue\n",
        "\n",
        "    image_id = int(filename.split(\".\")[0])\n",
        "    image_info = next((img for img in coco_data[\"images\"] if img[\"id\"] == image_id), None)\n",
        "    if not image_info:\n",
        "        logger.warning(f\"Image {filename} not found in annotations\")\n",
        "        continue\n",
        "\n",
        "    annotations = [ann for ann in coco_data[\"annotations\"] if ann[\"image_id\"] == image_id]\n",
        "    if not annotations:\n",
        "        logger.warning(f\"No annotations for {filename}\")\n",
        "        continue\n",
        "\n",
        "    suffix = \"\"\n",
        "    for ann in annotations:\n",
        "        category_name = category_map[ann[\"category_id\"]]\n",
        "        bbox = ann[\"bbox\"]\n",
        "        x1, y1 = bbox[0], bbox[1]\n",
        "        x2, y2 = x1 + bbox[2], y1 + bbox[3]\n",
        "        suffix += f\"{category_name};{x1},{y1},{x2},{y2};\"\n",
        "\n",
        "    paligemma_data.append({\n",
        "        \"image\": filename,\n",
        "        \"prefix\": \"detect object\",\n",
        "        \"suffix\": suffix.rstrip(\";\")\n",
        "    })\n",
        "\n",
        "# Step 8: Save PaliGemma JSONL\n",
        "jsonl_path = \"/content/coco_processed/paligemma_dataset.jsonl\"\n",
        "try:\n",
        "    with open(jsonl_path, \"w\") as f:\n",
        "        for entry in paligemma_data:\n",
        "            f.write(json.dumps(entry) + \"\\n\")\n",
        "    logger.info(f\"PaliGemma JSONL dataset saved to {jsonl_path}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error saving JSONL file: {e}\")\n",
        "    raise\n",
        "\n",
        "# Step 9: Load dataset manually\n",
        "try:\n",
        "    with open(jsonl_path, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "    dataset_list = [json.loads(line.strip()) for line in lines if line.strip()]\n",
        "    dataset = Dataset.from_list(dataset_list)\n",
        "    logger.info(f\"Dataset loaded with {len(dataset)} examples\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error loading dataset: {e}\")\n",
        "    raise\n",
        "\n",
        "# Step 10: Load PaliGemma model with quantization\n",
        "model_id = \"google/paligemma-3b-mix-448\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "try:\n",
        "    model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
        "        model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16\n",
        "    )\n",
        "    processor = AutoProcessor.from_pretrained(model_id)\n",
        "    logger.info(\"Model and processor loaded successfully\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error loading model or processor: {e}\")\n",
        "    raise\n",
        "\n",
        "# Step 11: Enable LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "logger.info(\"LoRA enabled; trainable parameters:\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Step 12: Preprocess dataset\n",
        "def preprocess_data(example):\n",
        "    try:\n",
        "        img_path = f\"/content/coco_images/{example['image']}\"\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        prompt = example[\"prefix\"]\n",
        "        suffix = example[\"suffix\"]\n",
        "\n",
        "        # Process inputs with PaliGemma processor\n",
        "        inputs = processor(text=prompt, images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        # Ensure inputs are tensors and squeeze dimensions\n",
        "        input_ids = inputs[\"input_ids\"].squeeze()\n",
        "        attention_mask = inputs[\"attention_mask\"].squeeze()\n",
        "        pixel_values = inputs[\"pixel_values\"].squeeze()\n",
        "\n",
        "        # Encode labels (suffix) as tensors\n",
        "        labels = processor.tokenizer.encode(suffix, add_special_tokens=False, return_tensors=\"pt\").squeeze()\n",
        "\n",
        "        # Validate tensor types\n",
        "        if not all(isinstance(x, torch.Tensor) for x in [input_ids, attention_mask, pixel_values, labels]):\n",
        "            logger.warning(f\"Invalid tensor types for {example['image']}\")\n",
        "            return None\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Error processing {example['image']}: {e}\")\n",
        "        return None\n",
        "\n",
        "processed_dataset = dataset.map(preprocess_data, remove_columns=dataset.column_names)\n",
        "processed_dataset = processed_dataset.filter(lambda x: x is not None)\n",
        "logger.info(f\"Processed dataset size: {len(processed_dataset)}\")\n",
        "\n",
        "# Step 13: Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/paligemma_finetuned\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=2,\n",
        "    logging_steps=5,\n",
        "    save_strategy=\"epoch\",\n",
        "    remove_unused_columns=False,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    bf16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    warmup_ratio=0.1,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Step 14: Custom collator\n",
        "def data_collator(examples):\n",
        "    if not examples or all(x is None for x in examples):\n",
        "        logger.warning(\"Empty or invalid batch in data_collator\")\n",
        "        return None\n",
        "\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    labels = []\n",
        "    pixel_values = []\n",
        "\n",
        "    for example in examples:\n",
        "        if example is None:\n",
        "            continue\n",
        "        if not all(isinstance(example[key], torch.Tensor) for key in [\"input_ids\", \"attention_mask\", \"labels\", \"pixel_values\"]):\n",
        "            logger.warning(f\"Skipping invalid example: {example}\")\n",
        "            continue\n",
        "        input_ids.append(example[\"input_ids\"])\n",
        "        attention_masks.append(example[\"attention_mask\"])\n",
        "        labels.append(example[\"labels\"])\n",
        "        pixel_values.append(example[\"pixel_values\"])\n",
        "\n",
        "    if not input_ids:\n",
        "        logger.warning(\"No valid examples in batch\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        return {\n",
        "            \"input_ids\": torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=processor.tokenizer.pad_token_id),\n",
        "            \"attention_mask\": torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0),\n",
        "            \"labels\": torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100),\n",
        "            \"pixel_values\": torch.stack(pixel_values)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in data_collator: {e}\")\n",
        "        return None\n",
        "\n",
        "# Step 15: Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=processed_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Step 16: Train the model\n",
        "try:\n",
        "    trainer.train()\n",
        "    logger.info(\"Training completed successfully\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error during training: {e}\")\n",
        "    raise\n",
        "\n",
        "# Step 17: Save the fine-tuned model\n",
        "model.save_pretrained(\"/content/paligemma_finetuned_model\")\n",
        "processor.save_pretrained(\"/content/paligemma_finetuned_model\")\n",
        "logger.info(\"Model and processor saved successfully\")\n",
        "\n",
        "# Step 18: Multilingual inference and object counting\n",
        "def detect_and_count_objects(image_path, prompt=\"detect object\", input_lang=\"en\", output_lang=\"en\"):\n",
        "    try:\n",
        "        # Initialize translator\n",
        "        if 'googletrans' in globals():\n",
        "            translator = Translator()\n",
        "            def translate_text(text, src, dest):\n",
        "                return translator.translate(text, src=src, dest=dest).text\n",
        "        else:\n",
        "            translator = FallbackTranslator(from_lang=input_lang, to_lang=output_lang)\n",
        "            def translate_text(text, src, dest):\n",
        "                return translator.translate(text)\n",
        "\n",
        "        # Translate prompt to English\n",
        "        if input_lang != \"en\":\n",
        "            prompt = translate_text(prompt, input_lang, \"en\")\n",
        "            logger.info(f\"Translated prompt from {input_lang} to en: {prompt}\")\n",
        "\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(**inputs, max_new_tokens=100)\n",
        "        decoded_output = processor.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "        # Parse detections\n",
        "        detections = [det.strip() for det in decoded_output.split(\";\") if det.strip()]\n",
        "        object_count = len(detections)\n",
        "\n",
        "        # Translate detections to output language\n",
        "        if output_lang != \"en\":\n",
        "            translated_detections = []\n",
        "            for det in detections:\n",
        "                parts = det.split(\";\")\n",
        "                if len(parts) >= 1:\n",
        "                    class_name = parts[0]\n",
        "                    coords = \";\".join(parts[1:]) if len(parts) > 1 else \"\"\n",
        "                    translated_class = translate_text(class_name, \"en\", output_lang)\n",
        "                    translated_detections.append(f\"{translated_class};{coords}\")\n",
        "                else:\n",
        "                    translated_detections.append(det)\n",
        "            detections = translated_detections\n",
        "            decoded_output = \";\".join(detections)\n",
        "            logger.info(f\"Translated detections to {output_lang}\")\n",
        "\n",
        "        return decoded_output, object_count\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during inference: {e}\")\n",
        "        return \"\", 0\n",
        "\n",
        "# Step 19: Demo interface for competition\n",
        "def run_demo():\n",
        "    logger.info(\"Starting SmartVision Counter demo\")\n",
        "    test_images = [\"/content/coco_images/000000289343.jpg\", \"/content/coco_images/000000039769.jpg\"]\n",
        "    prompts = [\n",
        "        (\"detect object\", \"en\", \"en\", \"English\"),\n",
        "        (\"thola into\", \"zu\", \"zu\", \"Zulu\"),\n",
        "        (\"gundua kitu\", \"sw\", \"sw\", \"Swahili\")\n",
        "    ]\n",
        "\n",
        "    print(\"\\n=== SmartVision Counter Demo ===\")\n",
        "    print(\"Automated object counting for African retail, agriculture, and manufacturing\")\n",
        "    print(\"Multilingual support for Zulu and Swahili to empower African communities\")\n",
        "    print(\"Model: google/paligemma-3b-mix-448\\n\")\n",
        "\n",
        "    for test_image in test_images:\n",
        "        if os.path.exists(test_image):\n",
        "            logger.info(f\"Demo for image: {test_image}\")\n",
        "            print(f\"\\n=== Processing {test_image.split('/')[-1]} ===\")\n",
        "            for prompt, input_lang, output_lang, lang_name in prompts:\n",
        "                detections, count = detect_and_count_objects(test_image, prompt, input_lang, output_lang)\n",
        "                print(f\"\\nLanguage: {lang_name}\")\n",
        "                print(f\"Prompt: '{prompt}'\")\n",
        "                print(f\"Detections: {detections}\")\n",
        "                print(f\"Object count: {count}\")\n",
        "        else:\n",
        "            logger.warning(f\"Test image {test_image} not found\")\n",
        "            print(f\"Test image {test_image} not found\")\n",
        "\n",
        "# Run the demo\n",
        "run_demo()"
      ],
      "metadata": {
        "id": "OZiWmelBhzES"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}